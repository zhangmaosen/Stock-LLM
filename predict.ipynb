{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userroot/miniconda3/envs/time/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: \"/data/models/qwen2-7b-instruct/model-00001-of-00004.safetensors\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mdel\u001b[39;00m model\n\u001b[1;32m     10\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m---> 11\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/data/models/qwen2-7b-instruct\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[39m#load_in_8bit=True,\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39m#load_in_4bit=True,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39m#quantization_config=BitsAndBytesConfig(\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#    load_in_4bit=True,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m#    bnb_4bit_compute_dtype=torch.float16,\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39m/data/models/qwen2-7b-instruct\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/time/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/time/lib/python3.11/site-packages/transformers/modeling_utils.py:3754\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3744\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3745\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3747\u001b[0m     (\n\u001b[1;32m   3748\u001b[0m         model,\n\u001b[1;32m   3749\u001b[0m         missing_keys,\n\u001b[1;32m   3750\u001b[0m         unexpected_keys,\n\u001b[1;32m   3751\u001b[0m         mismatched_keys,\n\u001b[1;32m   3752\u001b[0m         offload_index,\n\u001b[1;32m   3753\u001b[0m         error_msgs,\n\u001b[0;32m-> 3754\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3755\u001b[0m         model,\n\u001b[1;32m   3756\u001b[0m         state_dict,\n\u001b[1;32m   3757\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3758\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3759\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3760\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3761\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3762\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3763\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3764\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3765\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3766\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3767\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3768\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3769\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3770\u001b[0m         gguf_path\u001b[39m=\u001b[39;49mgguf_path,\n\u001b[1;32m   3771\u001b[0m     )\n\u001b[1;32m   3773\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3774\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/time/lib/python3.11/site-packages/transformers/modeling_utils.py:4194\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4192\u001b[0m \u001b[39mif\u001b[39;00m shard_file \u001b[39min\u001b[39;00m disk_only_shard_files:\n\u001b[1;32m   4193\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 4194\u001b[0m state_dict \u001b[39m=\u001b[39m load_state_dict(shard_file, is_quantized\u001b[39m=\u001b[39;49mis_quantized)\n\u001b[1;32m   4196\u001b[0m \u001b[39m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[1;32m   4197\u001b[0m \u001b[39m# matching the weights in the model.\u001b[39;00m\n\u001b[1;32m   4198\u001b[0m mismatched_keys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   4199\u001b[0m     state_dict,\n\u001b[1;32m   4200\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4204\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   4205\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/time/lib/python3.11/site-packages/transformers/modeling_utils.py:508\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39mReads a PyTorch checkpoint file, returning properly formatted errors if they arise.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m checkpoint_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.safetensors\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m is_safetensors_available():\n\u001b[1;32m    507\u001b[0m     \u001b[39m# Check format of the archive\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(checkpoint_file, framework\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    509\u001b[0m         metadata \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mmetadata()\n\u001b[1;32m    510\u001b[0m     \u001b[39mif\u001b[39;00m metadata\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mflax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmlx\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: \"/data/models/qwen2-7b-instruct/model-00001-of-00004.safetensors\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] =  \"1,2,3\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, Qwen2Model, AutoModel\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "config = AutoConfig.from_pretrained(\"/data/models/qwen2-7b-instruct\")\n",
    "config.output_hidden_states = False\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/data/models/qwen2-7b-instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    "\n",
    "    #load_in_8bit=True,\n",
    "    #load_in_4bit=True,\n",
    "    #quantization_config=BitsAndBytesConfig(\n",
    "    #    load_in_4bit=True,\n",
    "    #    bnb_4bit_compute_dtype=torch.float16,\n",
    "\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/models/qwen2-7b-instruct\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''券商最新重仓股曝光，四大行业受青睐中国石化获融资买入0.30亿元，近三日累计买入1.14亿元中国石化(600028)2023年报点评：增储增产降本取得突破 成品油经销量大幅增长中国石化与道达尔能源公司签署协议 将共同生产可持续航空燃料“三桶油”年报业绩出炉： 营收承压背景下主动推进绿色转型中国石化济南石油分公司积极开展世界森林日志愿捐款活动马永生会见中银（香港）总裁孙煜中国石化与泰国商务部签署合作谅解备忘录美银证券：重申中国石油化工“买入”评级 目标价升至5.2港元港股异动 | 中石油(00857)涨超4%领涨石油股 “三桶油”去年净赚3455亿 坚持高比例分红2024年中国LNG冷能利用资源及利用现状分析 LNG冷能利用项目超过20个【组图】国家标准化管理委员会关于下达《家用燃气快速热水器》等27项强制性国家标准制修订计划及相关标准外文版计划的通知中石化在南京成立日化科技公司 注册资本10亿港股收盘(03.28) | 恒指收涨0.91% 科网股强势反弹 家电、石油、黄金股等表现亮眼去年日赚9.46亿元！三大石油央企大手笔分红近1800亿元中国石化又一精细化工项目开工中国石化又一精细化工项目开工  苯胺-橡胶助剂产业链项目落户福建古雷石化基地日赚9.46亿元！“三桶油”加速向新能源转型【中银化工】公司点评-中国石化（600028.SH）：经营业绩维持高位，分红比例再提升,\"要点清单：\n",
    "1. 7x24小时全球实时财经新闻直播。\n",
    "2. 481家上市公司年报已披露，券商重仓股如中国石化、藏格矿业、宝丰能源。\n",
    "3. 券商青睐有色、石化、电子、化工等行业，持仓市值过10亿。\n",
    "4. 年报密集披露，关注券商持股变动以洞悉市场动态。\n",
    "\n",
    "【评论】随着年报密集发布，券商持仓动向成为观察市场风向的重要窗口。有色、石化等周期行业受青睐，显示当前市场对这些板块的看好。投资者可借此跟踪券商的行业配置，以调整投资策略。要点清单：\n",
    "1. 中国石化3月27日两融数据显示，融资买入0.3亿，净卖出2449万，排名232。\n",
    "2. 连续3日（25-27日）买入额分别为0.26亿、0.58亿、0.3亿。\n",
    "3. 当日融券卖出12.26万股，净卖出8.06万股。\n",
    "\n",
    "【评论】近期，中国石化在融资市场呈现净卖出态势，尽管买入金额有所增加，但净流出仍显明显。投资者可能需要关注其背后可能的市场信号或公司基本面变化。同时，融券卖出量虽大，但净卖出数量相对有限，可能反映了市场对石化板块的谨慎态度。在股市回暖时，投资者开户和选择智能投顾工具如定投、条件单等显得尤为重要。要点清单：\n",
    "1. 中国石化2023年营收3.2万亿，净利润604亿，同比降9.87%，受油价波动影响。\n",
    "2. 增储增产降本策略取得突破，油气产量增长，现金操作成本降低。\n",
    "3. 成品油经销量增长，推动“油气氢电服”转型，非油业务提升。\n",
    "4. 现金分红比例高，2024年利润预测为682-804亿，维持买入评级。\n",
    "5. 风险提示：油价、储量、宏观经济、政策及行业监管等。\n",
    "\n",
    "【评论】这份报告详细分析了中国石化2023年的业绩表现，尤其是在油气增储、成本控制和市场拓展方面取得进展。公司通过优化运营和转型策略，成功应对了市场挑战。预计未来几年盈利稳健，但需关注国际油价和政策风险。维持买入评级。中国石化与道达尔合作生产23万吨可持续航空燃料，年产能达23万吨。生物航煤使用废弃油脂，碳排放比传统石油基燃料减少50%以上。中国石化拥有自主生物航煤技术，2009年成功研发，2022年实现规模化生产。这符合道达尔的能源转型战略，两国共同推进航空业的碳足迹减少。【评论】这标志着中国在生物航煤技术及规模化生产上取得了显著进展，对清洁能源航空市场有积极影响。要点清单：\n",
    "1. \"\"三桶油\"\"2023年业绩：中石油盈利1611亿，增长8.3%；中石化604亿，下降9.9%；中海油1238亿，下降12.6%。\n",
    "2. 国际油价下跌影响：营收下滑，但中国石油净利润增长。\n",
    "3. 增储上产应对油价：三大公司加大绿色低碳转型，如中国石油新能源业务规模化发展。\n",
    "4. 分红政策：三公司均计划高比例分红，中国石化派息比例达75%。\n",
    "5. 2024展望：计划增加资本支出，推进能源服务商建设。\n",
    "\n",
    "【评论】：2023年“三桶油”业绩受油价影响，但中国石油逆市增长，显示出较强韧性。公司积极转型，布局新能源并计划加大分红，显示出对股东回报的承诺。未来，随着绿色低碳转型和资本支出增加，行业动态值得关注。要点清单：\n",
    "1. 中国石化济南石油分公司组织青年员工参与“我为雪域高原植新绿”活动，通过云植树募捐，绿化拉萨南北山。\n",
    "2. 公司坚持生态优先，绿色发展，连续多年开展义务植树，展现员工风貌，提升凝聚力。\n",
    "3. 活动旨在提高环保意识，实践绿色可持续发展，为建设绿水青山贡献力量。\n",
    "4. 中国石化济南石油分公司积极履行社会责任，推动公司高质量发展。\n",
    "\n",
    "【评论】该活动不仅是企业社会责任的体现，也是践行绿色发展理念的生动实践。通过这样的方式，企业不仅绿化环境，还提升了员工的环保意识，对公司的长远发展和公众形象都有积极影响。要点清单：\n",
    "1. 中国石化董事长马永生会晤中银（香港）总裁孙煜。\n",
    "2. 双方深化资金池、现金管理、存贷款、贸易融资和外汇合作。\n",
    "3. 讨论合作拓展至东南亚及一带一路沿线国家。\n",
    "4. 目标是高质量共建一带一路。\n",
    "5. 副总经理喻宝才也参与了会见。\n",
    "\n",
    "【评论】这次会晤显示了中石化与中银在金融领域的深化合作意愿，特别是在一带一路项目中的战略协同。双方计划将合作范围扩大到东南亚，这将有利于企业国际化和区域经济一体化，对能源和金融行业具有积极影响。要点清单：\n",
    "1. 中国石化与泰国商务部签署合作备忘录，深化产品推广和市场开拓。\n",
    "2. 双方强调在\"\"一带一路\"\"框架下的合作，目标是扩大贸易和投资交流。\n",
    "3. 合作将促进业务合作，提供两国优质产品，满足民众生活需求。\n",
    "4. 赵东与泰国国家石油公司总裁讨论了多个领域合作机会。\n",
    "5. 交流还包括拜会中国大使馆和调研合资加油站。\n",
    "\n",
    "【评论】此合作标志着中泰两国在能源和商业领域的深化合作，有利于双方互利共赢，同时也为\"\"一带一路\"\"倡议增添了实际操作案例。中国石化在泰国的业务拓展显示出其全球化布局的策略，未来值得关注其具体合作成果和市场反应。要点清单：\n",
    "1. 美银证券维持中国石油化工\"\"买入\"\"评级，上调目标价至5.2港元。\n",
    "2. 管理层预计2024财年油气、成品油需求正常化，但化工产能过剩，利润微薄。\n",
    "3. 国企估值重评背景下，化工行业面临产能过剩与成本压力。\n",
    "4. 管理层对化工市场持谨慎态度，期待利润逐步恢复。\n",
    "\n",
    "【评论】这份报告关注了中国石油化工的市场前景和盈利预期，尤其是在当前行业产能过剩和成本上升的背景下。投资者应关注管理层的策略调整和市场恢复进程，以做出更明智的投资决策。要点清单：\n",
    "1. 石油股午后涨幅扩大，中石油、中海油服等公司股价上涨。\n",
    "2. \"\"三桶油\"\"2023年业绩稳健，营收下滑但利润仍高，净利润合计3454.5亿。\n",
    "3. 在油价下降背景下，公司坚持高分红策略，中石油派息率50%。\n",
    "4. 市场分析师看好\"\"三桶油\"\"在油价波动中的稳定收益潜力。\n",
    "\n",
    "【评论】近期石油股受业绩提振及分红政策吸引，市场反应积极。\"\"三桶油\"\"的稳健经营和高分红策略为投资者提供了一定的防御性。然而，投资者还需关注油价走势对业绩的影响，以及行业整体环境变化。中国已建成27座LNG接收站，接收规模大。然而，LNG冷能利用率仅10%-25%，且主要为空气分离，发电项目未实现。新奥股份的自主研发项目尝试冷能发电，填补了空白。近20个冷能利用项目在建设，但技术以空气分离为主，节能减排推动冷能利用。【评论】对于提升LNG产业的能源效率和环保价值，新奥的创新实践值得关注，未来有望推动行业技术进步。【总结】国家标准化管理委员会发布了27项强制性国家标准修订计划，涉及家用燃气快速热水器、殡葬服务传染病防控、消防设备等多个领域。这些标准的修订旨在保证产品质量和技术水平，强化安全监管。各相关部门和企业需参与起草、征求意见和技术审查，确保按时完成任务。\n",
    "\n",
    "【评论】这份标准修订计划凸显了国家对关键行业安全和民生质量的重视。家用燃气热水器的安全标准升级，将保障消费者使用安全；殡葬服务标准的制定，反映了社会对防控疫情的最新要求。消防设备标准的修订则强化了公共安全防护。这些标准的更新对于推动行业规范化和提升国家整体安全水平具有重要意义。要点清单：\n",
    "1. 24/7全球实时财经新闻直播。\n",
    "2. 中石化金陵日化科技成立，注册资本10亿，张春生为法定代表人。\n",
    "3. 股东包括中石化和金陵石化，专注基础化学、专用化学制品等制造。\n",
    "4. 新闻来自天眼查App，用户可通过新浪财经APP获取。\n",
    "\n",
    "【评论】作为投研分析人员，这个信息对石化行业动态有重要价值。中石化金陵日化科技的成立，可能涉及产业链整合或新产品开发，值得关注其后续运营和市场表现。实时财经直播则确保了投资者能及时掌握行业动态，对投资决策有指导意义。要点清单：\n",
    "1. 港股反弹，恒生指数涨0.91%，科技股领涨，中升控股涨幅显著。\n",
    "2. 建银国际预期市场反弹受财报季影响，盈利前景不明朗，未来动力可能减弱。\n",
    "3. 家电股受益废弃电器电子产品处理补贴政策，海尔智家涨幅领先。\n",
    "4. 石油股因业绩及降息预期上涨，中石油创历史新高。\n",
    "5. 有色与黄金股受避险情绪及国际金价上涨推动，灵宝黄金涨幅显著。\n",
    "6. 餐饮股复苏，商务部政策支持行业高质量发展。\n",
    "7. 中国中免与光大银行业绩表现分化，中免强劲，光大银行利润下滑。\n",
    "\n",
    "【评论】近期港股市场反弹明显，受益行业如家电、科技及石油股表现强劲，但盈利预期及市场动力的不确定性仍存。政策支持的家电和餐饮行业展现出复苏势头，而银行板块则面临业绩压力。投资者需关注行业动态与公司财报，以把握市场脉搏。要点清单：\n",
    "1. 中国三大石油公司2022年共盈利3454.5亿，日赚9.46亿。\n",
    "2. 中国石油净利润创历史新高，营收和利润均创新高。\n",
    "3. 国有企业改革鼓励分红，三家公司分红总额1757.79亿。\n",
    "4. 现金流强，中国海油现金流达888.7亿，显示稳健财务。\n",
    "5. 分红策略吸引长线投资者，预计2024年业绩稳中有升。\n",
    "\n",
    "【评论】：中国石油的强劲业绩和分红计划显示出其在能源行业中的主导地位和对股东的承诺。随着国企改革的推进，稳定分红可能成为常态，这不仅有利于投资者，也体现了公司对未来的积极预期。未来几年，随着油气行业需求稳定和成本控制，三家公司有望实现业绩增长和持续分红，为投资者带来长期稳定收益。要点清单：\n",
    "1. 中国石化南化公司苯胺-橡胶助剂产业链项目在福建古雷石化基地开工，总投资大，涵盖30万吨苯胺、8.6万吨橡胶助剂等。\n",
    "2. 项目旨在提升石化产业链，助力古雷园区发展，体现中国石化与福建省深化合作。\n",
    "3. 项目基于南化公司国内领先的技术和产业链优势，苯胺市场占有率第一。\n",
    "4. 中国石化计划打造古雷产业基地，目标是高端智能绿色。\n",
    "\n",
    "【评论】：中国石化南化公司新项目的启动，标志着公司在产业链优化和区域合作上迈出了重要步伐。该项目结合南化公司的技术优势和古雷石化基地的产业基础，有望推动石化产业高质量发展，同时也显示出公司对苯胺和橡胶助剂市场的深入理解和领先竞争力。要点清单：\n",
    "1. 中国石化南化公司启动苯胺-橡胶助剂产业链项目，位于福建古雷石化基地。\n",
    "2. 项目包括30万吨苯胺、8.6万吨橡胶助剂及MIBK，旨在提升产业链质量。\n",
    "3. 项目被视为中国石化与福建深化合作的里程碑，目标打造绿色、高端石化基地。\n",
    "4. 南化公司拥有国内唯一完整苯胺-橡胶助剂产业链，产品广泛应用于染料和橡胶制品。\n",
    "5. 项目将助力古雷石化园区产业发展，南化公司有70余年行业经验，技术领先。\n",
    "\n",
    "【评论】此项目不仅彰显了中国石化在产业链整合上的决心，也凸显了南化公司在苯胺和橡胶助剂领域的行业领导地位。福建古雷基地的建设将推动当地及全国石化产业的高质量发展，同时南化公司的转型升级也体现了中国化学工业的绿色转型与创新。要点清单：\n",
    "1. \"\"三桶油\"\"2023年业绩受国际油价下跌影响，营收总计6.65万亿，净利润3453.6亿。\n",
    "2. 中国石油净利润创历史新高，但中国石化和中国海油分别下降9.87%和12.6%。\n",
    "3. 三公司加速新能源布局，应对低碳发展和市场波动风险。\n",
    "4. 中国石油在风电、氢能源、CCUS上有显著进展，中国石化聚焦氢能，中国海油推动新能源与海上油气融合。\n",
    "\n",
    "【评论】：中国石油在新能源转型中走在前列，中国石化和中国海油也积极布局，显示出行业对绿色低碳转型的响应。尽管传统业务面临挑战，但这些公司的积极调整和新能源业务的发展前景值得关注。要点清单：\n",
    "1. 公司营收3212亿，同比减少，四季度尤其显著，净利润605亿，同比降9.87%。\n",
    "2. 扣非净利润增长，但受非经常性项目影响，归母净利润下滑。\n",
    "3. 上游油气产量增长，增储降本策略成效，但油价下跌影响销售价格。\n",
    "4. 炼销业务一体化，产品结构调整，炼油盈利提升，化工亏损减缩。\n",
    "5. 资本支出预计降低，分红比例提升至72.1%。\n",
    "6. 风险提示包括油价波动、安全生产和全球经济等。\n",
    "\n",
    "【评论】这份报告提供了公司2023年度关键业绩和未来展望，显示出公司在成本控制和结构调整方面的努力。尽管面临油价下跌挑战，油气业务仍保持增长，而炼油业务有望随着需求恢复盈利。分红比例的提升显示公司对股东信心。投资者应关注行业风险和全球经济环境。\"\n",
    "'''\n",
    "print(f'prompt lenght is {len(prompt)}')\n",
    "#prompt = prompt[:4096]\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text, text, text], return_tensors=\"pt\").to(device)\n",
    "print(model_inputs.input_ids.shape)\n",
    "# output = model.generate(\n",
    "#     model_inputs.input_ids,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, output)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'output' in locals():\n",
    "    del output\n",
    "    torch.cuda.empty_cache()\n",
    "output = model(input_ids=model_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "del output\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
